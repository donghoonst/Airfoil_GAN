{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\working_space\\airfoil_project\\airfoil에서 29개의 이미지를 로드했습니다.\n",
      "이미지 데이터의 형태: (29, 64, 64, 3)\n",
      "첫 번째 이미지의 최대 값: 1.0, 최소 값: -1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 데이터의 평균: 0.9800102114677429, 표준편차: 0.1815299689769745\n",
      "이미지 데이터의 최대 값: 1.0, 최소 값: -1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "# 이미지 로드 및 전처리 함수\n",
    "def load_images(folder, image_size=(64, 64)):\n",
    "    images = []\n",
    "    for img_path in glob(os.path.join(folder, '*.png')):\n",
    "        img = load_img(img_path, target_size=image_size)\n",
    "        img = img_to_array(img)\n",
    "        img = (img - 127.5) / 127.5  # [-1, 1]로 정규화\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# 데이터 폴더 및 이미지 크기 설정\n",
    "image_folder = \"C:\\\\working_space\\\\airfoil_project\\\\airfoil\"\n",
    "image_size = (64, 64)\n",
    "\n",
    "# 이미지 로드\n",
    "images = load_images(image_folder, image_size)\n",
    "print(f\"{image_folder}에서 {len(images)}개의 이미지를 로드했습니다.\")\n",
    "\n",
    "# 데이터의 구성 확인\n",
    "print(f\"이미지 데이터의 형태: {images.shape}\")\n",
    "print(f\"첫 번째 이미지의 최대 값: {np.max(images[0])}, 최소 값: {np.min(images[0])}\")\n",
    "\n",
    "# 샘플 이미지 시각화\n",
    "def plot_sample_images(images, num_images=5):\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i in range(num_images):\n",
    "        ax = plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow((images[i] * 0.5 + 0.5))  # [0, 1]로 재조정\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# 샘플 이미지 시각화\n",
    "plot_sample_images(images)\n",
    "\n",
    "# 데이터 통계\n",
    "print(f\"이미지 데이터의 평균: {np.mean(images)}, 표준편차: {np.std(images)}\")\n",
    "print(f\"이미지 데이터의 최대 값: {np.max(images)}, 최소 값: {np.min(images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 258ms/step\n",
      "WARNING:tensorflow:5 out of the last 1693 calls to <function Model.make_train_function.<locals>.train_function at 0x0000017044D79040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0 [D loss: 1.268731415271759, acc.: 21.875] [G loss: 0.7364834547042847]\n",
      "WARNING:tensorflow:5 out of the last 572 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001705D014EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1 [D loss: 0.8567885756492615, acc.: 56.25] [G loss: 0.8640117645263672]\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "2 [D loss: 0.41711558401584625, acc.: 81.25] [G loss: 1.1331452131271362]\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "3 [D loss: 0.43237361311912537, acc.: 82.8125] [G loss: 1.4301443099975586]\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "4 [D loss: 0.4857262969017029, acc.: 78.125] [G loss: 1.688774585723877]\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "5 [D loss: 0.39198657870292664, acc.: 82.8125] [G loss: 1.9697563648223877]\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "6 [D loss: 0.37651170790195465, acc.: 89.0625] [G loss: 2.280282974243164]\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "7 [D loss: 0.339456707239151, acc.: 87.5] [G loss: 2.724275588989258]\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "8 [D loss: 0.7428502142429352, acc.: 53.125] [G loss: 2.962984800338745]\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "9 [D loss: 0.6496086716651917, acc.: 62.5] [G loss: 2.4659876823425293]\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "10 [D loss: 0.7300807535648346, acc.: 48.4375] [G loss: 2.9514575004577637]\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "11 [D loss: 0.36348628997802734, acc.: 85.9375] [G loss: 3.00484037399292]\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "12 [D loss: 0.43377457559108734, acc.: 78.125] [G loss: 3.102372646331787]\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "13 [D loss: 0.24264392256736755, acc.: 95.3125] [G loss: 1.7598271369934082]\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "14 [D loss: 0.19958296418190002, acc.: 96.875] [G loss: 1.6499874591827393]\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "15 [D loss: 0.15955949574708939, acc.: 98.4375] [G loss: 1.6902990341186523]\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "16 [D loss: 0.22167296707630157, acc.: 92.1875] [G loss: 1.3470842838287354]\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "17 [D loss: 0.18070544302463531, acc.: 98.4375] [G loss: 1.661417841911316]\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "18 [D loss: 0.3500216379761696, acc.: 84.375] [G loss: 2.1630663871765137]\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "19 [D loss: 0.31898432970046997, acc.: 90.625] [G loss: 1.2768771648406982]\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "20 [D loss: 0.6127399206161499, acc.: 68.75] [G loss: 2.674891471862793]\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "21 [D loss: 0.6652731895446777, acc.: 67.1875] [G loss: 2.6500911712646484]\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "22 [D loss: 0.6089289635419846, acc.: 71.875] [G loss: 2.996776580810547]\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "23 [D loss: 0.37586189806461334, acc.: 81.25] [G loss: 2.9165689945220947]\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "24 [D loss: 0.27152755856513977, acc.: 89.0625] [G loss: 2.296403169631958]\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "25 [D loss: 0.26665832102298737, acc.: 87.5] [G loss: 1.6784865856170654]\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "26 [D loss: 0.27955105900764465, acc.: 90.625] [G loss: 1.6572147607803345]\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "27 [D loss: 0.32083775103092194, acc.: 87.5] [G loss: 2.2341268062591553]\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "28 [D loss: 0.5959489792585373, acc.: 67.1875] [G loss: 2.9291207790374756]\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "29 [D loss: 0.468623548746109, acc.: 76.5625] [G loss: 3.5661654472351074]\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "30 [D loss: 0.49753665924072266, acc.: 75.0] [G loss: 2.637702465057373]\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "31 [D loss: 0.5172631740570068, acc.: 78.125] [G loss: 2.8719563484191895]\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "32 [D loss: 0.37302157282829285, acc.: 82.8125] [G loss: 3.239459276199341]\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "33 [D loss: 0.36936530470848083, acc.: 78.125] [G loss: 2.4844982624053955]\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "34 [D loss: 0.12604593485593796, acc.: 98.4375] [G loss: 3.018444299697876]\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "35 [D loss: 0.20086155086755753, acc.: 95.3125] [G loss: 2.612384796142578]\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "36 [D loss: 0.298519492149353, acc.: 87.5] [G loss: 2.5933868885040283]\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "37 [D loss: 0.21241243928670883, acc.: 93.75] [G loss: 2.8951988220214844]\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "38 [D loss: 0.16064385324716568, acc.: 95.3125] [G loss: 2.654083013534546]\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "39 [D loss: 0.18538250029087067, acc.: 95.3125] [G loss: 2.8589470386505127]\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "40 [D loss: 0.19187362492084503, acc.: 96.875] [G loss: 2.7421796321868896]\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "41 [D loss: 0.3897201269865036, acc.: 84.375] [G loss: 3.041081190109253]\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "42 [D loss: 0.18277467787265778, acc.: 93.75] [G loss: 3.456367015838623]\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "43 [D loss: 0.27885018289089203, acc.: 87.5] [G loss: 2.546675682067871]\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "44 [D loss: 0.1669388711452484, acc.: 96.875] [G loss: 3.1689467430114746]\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "45 [D loss: 0.21967294812202454, acc.: 95.3125] [G loss: 2.9561004638671875]\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "46 [D loss: 0.3447195887565613, acc.: 82.8125] [G loss: 3.0121045112609863]\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "47 [D loss: 0.17877794802188873, acc.: 95.3125] [G loss: 3.94852876663208]\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "48 [D loss: 0.24268677830696106, acc.: 92.1875] [G loss: 3.0726516246795654]\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "49 [D loss: 0.16939151287078857, acc.: 98.4375] [G loss: 3.1149487495422363]\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "50 [D loss: 0.2018675059080124, acc.: 95.3125] [G loss: 2.9771571159362793]\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "51 [D loss: 0.17722565680742264, acc.: 96.875] [G loss: 3.207125425338745]\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "52 [D loss: 0.27409224212169647, acc.: 87.5] [G loss: 2.717409133911133]\n",
      "1/1 [==============================] - 0s 405ms/step\n",
      "53 [D loss: 0.21514736860990524, acc.: 95.3125] [G loss: 4.4566473960876465]\n",
      "1/1 [==============================] - 0s 445ms/step\n",
      "54 [D loss: 0.21221645176410675, acc.: 93.75] [G loss: 3.6908106803894043]\n",
      "1/1 [==============================] - 0s 387ms/step\n",
      "55 [D loss: 0.11810698360204697, acc.: 98.4375] [G loss: 2.7818045616149902]\n",
      "1/1 [==============================] - 0s 454ms/step\n",
      "56 [D loss: 0.17058808356523514, acc.: 95.3125] [G loss: 3.409486770629883]\n",
      "1/1 [==============================] - 0s 415ms/step\n",
      "57 [D loss: 0.21686507016420364, acc.: 90.625] [G loss: 3.248136520385742]\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "58 [D loss: 0.11294902488589287, acc.: 98.4375] [G loss: 3.881124973297119]\n",
      "1/1 [==============================] - 0s 447ms/step\n",
      "59 [D loss: 0.16873947530984879, acc.: 95.3125] [G loss: 3.145684003829956]\n",
      "1/1 [==============================] - 1s 542ms/step\n",
      "60 [D loss: 0.3381558358669281, acc.: 82.8125] [G loss: 4.620952606201172]\n",
      "1/1 [==============================] - 0s 467ms/step\n",
      "61 [D loss: 0.1648099049925804, acc.: 93.75] [G loss: 3.9962587356567383]\n",
      "1/1 [==============================] - 0s 419ms/step\n",
      "62 [D loss: 0.2174379751086235, acc.: 95.3125] [G loss: 3.425841808319092]\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "63 [D loss: 0.1414593905210495, acc.: 96.875] [G loss: 4.133415222167969]\n",
      "1/1 [==============================] - 0s 429ms/step\n",
      "64 [D loss: 0.16930435970425606, acc.: 93.75] [G loss: 3.4746365547180176]\n",
      "1/1 [==============================] - 0s 427ms/step\n",
      "65 [D loss: 0.11782068386673927, acc.: 96.875] [G loss: 2.291602849960327]\n",
      "1/1 [==============================] - 0s 426ms/step\n",
      "66 [D loss: 0.13831653632223606, acc.: 95.3125] [G loss: 4.395627975463867]\n",
      "1/1 [==============================] - 0s 429ms/step\n",
      "67 [D loss: 0.24099749326705933, acc.: 89.0625] [G loss: 4.603851318359375]\n",
      "1/1 [==============================] - 0s 458ms/step\n",
      "68 [D loss: 0.4287167340517044, acc.: 76.5625] [G loss: 4.067238807678223]\n",
      "1/1 [==============================] - 1s 512ms/step\n",
      "69 [D loss: 0.1834583654999733, acc.: 95.3125] [G loss: 3.964627981185913]\n",
      "1/1 [==============================] - 0s 471ms/step\n",
      "70 [D loss: 0.1828961968421936, acc.: 93.75] [G loss: 3.5707011222839355]\n",
      "1/1 [==============================] - 0s 423ms/step\n",
      "71 [D loss: 0.22080354392528534, acc.: 92.1875] [G loss: 4.232025146484375]\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "72 [D loss: 0.19579843431711197, acc.: 95.3125] [G loss: 3.5180511474609375]\n",
      "1/1 [==============================] - 0s 477ms/step\n",
      "73 [D loss: 0.16473166644573212, acc.: 95.3125] [G loss: 3.7505431175231934]\n",
      "1/1 [==============================] - 0s 408ms/step\n",
      "74 [D loss: 0.10258856788277626, acc.: 98.4375] [G loss: 3.315396547317505]\n",
      "1/1 [==============================] - 1s 518ms/step\n",
      "75 [D loss: 0.13610448688268661, acc.: 96.875] [G loss: 3.7686707973480225]\n",
      "1/1 [==============================] - 1s 510ms/step\n",
      "76 [D loss: 0.12127430737018585, acc.: 95.3125] [G loss: 3.049086332321167]\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "77 [D loss: 0.23243770748376846, acc.: 90.625] [G loss: 3.8466598987579346]\n",
      "1/1 [==============================] - 0s 419ms/step\n",
      "78 [D loss: 0.18508030474185944, acc.: 95.3125] [G loss: 3.453885078430176]\n",
      "1/1 [==============================] - 0s 474ms/step\n",
      "79 [D loss: 0.1464550644159317, acc.: 96.875] [G loss: 3.294203758239746]\n",
      "1/1 [==============================] - 0s 417ms/step\n",
      "80 [D loss: 0.15667495131492615, acc.: 98.4375] [G loss: 2.860771656036377]\n",
      "1/1 [==============================] - 1s 532ms/step\n",
      "81 [D loss: 0.14294162392616272, acc.: 96.875] [G loss: 3.259918212890625]\n",
      "1/1 [==============================] - 0s 443ms/step\n",
      "82 [D loss: 0.1333940103650093, acc.: 98.4375] [G loss: 2.980377435684204]\n",
      "1/1 [==============================] - 0s 470ms/step\n",
      "83 [D loss: 0.1132943257689476, acc.: 96.875] [G loss: 3.001976490020752]\n",
      "1/1 [==============================] - 0s 458ms/step\n",
      "84 [D loss: 0.15902365744113922, acc.: 93.75] [G loss: 2.758000373840332]\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "85 [D loss: 0.25998862087726593, acc.: 92.1875] [G loss: 3.475499391555786]\n",
      "1/1 [==============================] - 0s 473ms/step\n",
      "86 [D loss: 0.43782973289489746, acc.: 78.125] [G loss: 3.1551835536956787]\n",
      "1/1 [==============================] - 0s 407ms/step\n",
      "87 [D loss: 0.167917400598526, acc.: 93.75] [G loss: 3.7326996326446533]\n",
      "1/1 [==============================] - 0s 469ms/step\n",
      "88 [D loss: 0.2897116020321846, acc.: 87.5] [G loss: 1.953190803527832]\n",
      "1/1 [==============================] - 1s 539ms/step\n",
      "89 [D loss: 0.06761396303772926, acc.: 98.4375] [G loss: 1.8754093647003174]\n",
      "1/1 [==============================] - 0s 446ms/step\n",
      "90 [D loss: 0.15727251768112183, acc.: 95.3125] [G loss: 3.075788974761963]\n",
      "1/1 [==============================] - 0s 480ms/step\n",
      "91 [D loss: 0.1265755370259285, acc.: 95.3125] [G loss: 3.414512872695923]\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "92 [D loss: 0.13332033902406693, acc.: 98.4375] [G loss: 2.8587305545806885]\n",
      "1/1 [==============================] - 0s 445ms/step\n",
      "93 [D loss: 0.25968096405267715, acc.: 85.9375] [G loss: 3.016876697540283]\n",
      "1/1 [==============================] - 0s 457ms/step\n",
      "94 [D loss: 0.07772180251777172, acc.: 98.4375] [G loss: 2.503242254257202]\n",
      "1/1 [==============================] - 0s 494ms/step\n",
      "95 [D loss: 0.11408737301826477, acc.: 98.4375] [G loss: 2.7643423080444336]\n",
      "1/1 [==============================] - 0s 439ms/step\n",
      "96 [D loss: 0.05357184819877148, acc.: 98.4375] [G loss: 2.5805256366729736]\n",
      "1/1 [==============================] - 0s 420ms/step\n",
      "97 [D loss: 0.10357771813869476, acc.: 98.4375] [G loss: 2.3905177116394043]\n",
      "1/1 [==============================] - 0s 450ms/step\n",
      "98 [D loss: 0.09682793542742729, acc.: 98.4375] [G loss: 3.412837028503418]\n",
      "1/1 [==============================] - 0s 414ms/step\n",
      "99 [D loss: 0.13175884261727333, acc.: 98.4375] [G loss: 2.096294641494751]\n",
      "1/1 [==============================] - 0s 430ms/step\n",
      "100 [D loss: 0.24635111540555954, acc.: 89.0625] [G loss: 4.388554573059082]\n",
      "1/1 [==============================] - 0s 346ms/step\n",
      "1/1 [==============================] - 0s 488ms/step\n",
      "101 [D loss: 0.13507818430662155, acc.: 96.875] [G loss: 3.819624423980713]\n",
      "1/1 [==============================] - 0s 483ms/step\n",
      "102 [D loss: 0.13375625014305115, acc.: 96.875] [G loss: 1.632391095161438]\n",
      "1/1 [==============================] - 0s 445ms/step\n",
      "103 [D loss: 0.1532931663095951, acc.: 93.75] [G loss: 3.6939127445220947]\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "104 [D loss: 0.12012719176709652, acc.: 96.875] [G loss: 2.990814208984375]\n",
      "1/1 [==============================] - 1s 523ms/step\n",
      "105 [D loss: 0.223877914249897, acc.: 89.0625] [G loss: 2.419480085372925]\n",
      "1/1 [==============================] - 1s 594ms/step\n",
      "106 [D loss: 0.05838736146688461, acc.: 100.0] [G loss: 3.2429916858673096]\n",
      "1/1 [==============================] - 1s 536ms/step\n",
      "107 [D loss: 0.05256387125700712, acc.: 100.0] [G loss: 2.5430314540863037]\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "108 [D loss: 0.07789032720029354, acc.: 98.4375] [G loss: 1.7959177494049072]\n",
      "1/1 [==============================] - 0s 412ms/step\n",
      "109 [D loss: 0.08324936963617802, acc.: 96.875] [G loss: 3.052912950515747]\n",
      "1/1 [==============================] - 0s 387ms/step\n",
      "110 [D loss: 0.17524412274360657, acc.: 93.75] [G loss: 3.4635276794433594]\n",
      "1/1 [==============================] - 1s 542ms/step\n",
      "111 [D loss: 0.11336952447891235, acc.: 96.875] [G loss: 3.1658427715301514]\n",
      "1/1 [==============================] - 0s 417ms/step\n",
      "112 [D loss: 0.18840859830379486, acc.: 92.1875] [G loss: 3.8063807487487793]\n",
      "1/1 [==============================] - 1s 548ms/step\n",
      "113 [D loss: 0.11383074894547462, acc.: 96.875] [G loss: 2.9982266426086426]\n",
      "1/1 [==============================] - 0s 495ms/step\n",
      "114 [D loss: 0.176284559071064, acc.: 95.3125] [G loss: 1.9712926149368286]\n",
      "1/1 [==============================] - 1s 537ms/step\n",
      "115 [D loss: 0.06991051882505417, acc.: 100.0] [G loss: 1.6626243591308594]\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "116 [D loss: 0.15922117978334427, acc.: 96.875] [G loss: 3.6690731048583984]\n",
      "1/1 [==============================] - 0s 436ms/step\n",
      "117 [D loss: 0.10239630192518234, acc.: 96.875] [G loss: 3.241262912750244]\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "118 [D loss: 0.3415989428758621, acc.: 85.9375] [G loss: 1.1518921852111816]\n",
      "1/1 [==============================] - 0s 483ms/step\n",
      "119 [D loss: 0.08721989765763283, acc.: 98.4375] [G loss: 2.218776226043701]\n",
      "1/1 [==============================] - 0s 480ms/step\n",
      "120 [D loss: 0.08228908851742744, acc.: 98.4375] [G loss: 2.009990692138672]\n",
      "1/1 [==============================] - 0s 492ms/step\n",
      "121 [D loss: 0.1174200139939785, acc.: 96.875] [G loss: 3.0841407775878906]\n",
      "1/1 [==============================] - 0s 478ms/step\n",
      "122 [D loss: 0.15207716822624207, acc.: 98.4375] [G loss: 2.1974573135375977]\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "123 [D loss: 0.364146426320076, acc.: 79.6875] [G loss: 1.5574100017547607]\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "124 [D loss: 0.14835818111896515, acc.: 96.875] [G loss: 3.2154736518859863]\n",
      "1/1 [==============================] - 1s 736ms/step\n",
      "125 [D loss: 0.05885800160467625, acc.: 98.4375] [G loss: 3.0250468254089355]\n",
      "1/1 [==============================] - 0s 433ms/step\n",
      "126 [D loss: 0.09333130600862205, acc.: 96.875] [G loss: 0.7118672132492065]\n",
      "1/1 [==============================] - 0s 473ms/step\n",
      "127 [D loss: 0.13187645561993122, acc.: 95.3125] [G loss: 1.7010562419891357]\n",
      "1/1 [==============================] - 0s 469ms/step\n",
      "128 [D loss: 0.05193295516073704, acc.: 100.0] [G loss: 2.191466808319092]\n",
      "1/1 [==============================] - 0s 495ms/step\n",
      "129 [D loss: 0.14238177239894867, acc.: 95.3125] [G loss: 1.7170841693878174]\n",
      "1/1 [==============================] - 0s 438ms/step\n",
      "130 [D loss: 0.053856274113059044, acc.: 100.0] [G loss: 2.418644428253174]\n",
      "1/1 [==============================] - 0s 418ms/step\n",
      "131 [D loss: 0.11428331024944782, acc.: 96.875] [G loss: 1.1569292545318604]\n",
      "1/1 [==============================] - 0s 384ms/step\n",
      "132 [D loss: 0.09980878792703152, acc.: 96.875] [G loss: 2.618986129760742]\n",
      "1/1 [==============================] - 1s 739ms/step\n",
      "133 [D loss: 0.042368996888399124, acc.: 100.0] [G loss: 3.2223896980285645]\n",
      "1/1 [==============================] - 0s 410ms/step\n",
      "134 [D loss: 0.0889153927564621, acc.: 96.875] [G loss: 1.1045094728469849]\n",
      "1/1 [==============================] - 0s 409ms/step\n",
      "135 [D loss: 0.05067779682576656, acc.: 98.4375] [G loss: 1.6907862424850464]\n",
      "1/1 [==============================] - 0s 471ms/step\n",
      "136 [D loss: 0.10057184472680092, acc.: 93.75] [G loss: 3.215317726135254]\n",
      "1/1 [==============================] - 0s 407ms/step\n",
      "137 [D loss: 0.11259759962558746, acc.: 96.875] [G loss: 1.6570265293121338]\n",
      "1/1 [==============================] - 0s 386ms/step\n",
      "138 [D loss: 0.07238395884633064, acc.: 98.4375] [G loss: 2.1931276321411133]\n",
      "1/1 [==============================] - 1s 612ms/step\n",
      "139 [D loss: 0.05305648595094681, acc.: 100.0] [G loss: 2.3431875705718994]\n",
      "1/1 [==============================] - 0s 496ms/step\n",
      "140 [D loss: 0.10774904116988182, acc.: 95.3125] [G loss: 0.9958069920539856]\n",
      "1/1 [==============================] - 0s 405ms/step\n",
      "141 [D loss: 0.05633680988103151, acc.: 98.4375] [G loss: 1.4746171236038208]\n",
      "1/1 [==============================] - 1s 508ms/step\n",
      "142 [D loss: 0.08418391644954681, acc.: 100.0] [G loss: 1.8743674755096436]\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "143 [D loss: 0.16748446971178055, acc.: 90.625] [G loss: 1.637410044670105]\n",
      "1/1 [==============================] - 1s 608ms/step\n",
      "144 [D loss: 0.03577708639204502, acc.: 98.4375] [G loss: 1.9836199283599854]\n",
      "1/1 [==============================] - 0s 458ms/step\n",
      "145 [D loss: 0.032791635021567345, acc.: 100.0] [G loss: 1.6910282373428345]\n",
      "1/1 [==============================] - 1s 545ms/step\n",
      "146 [D loss: 0.06964051723480225, acc.: 100.0] [G loss: 2.469118118286133]\n",
      "1/1 [==============================] - 1s 676ms/step\n",
      "147 [D loss: 0.07550030201673508, acc.: 98.4375] [G loss: 1.6583011150360107]\n",
      "1/1 [==============================] - 0s 386ms/step\n",
      "148 [D loss: 0.12883833050727844, acc.: 96.875] [G loss: 1.6736235618591309]\n",
      "1/1 [==============================] - 1s 890ms/step\n",
      "149 [D loss: 0.03325403295457363, acc.: 100.0] [G loss: 1.5460766553878784]\n",
      "1/1 [==============================] - 1s 546ms/step\n",
      "150 [D loss: 0.19322264194488525, acc.: 92.1875] [G loss: 2.761843681335449]\n",
      "1/1 [==============================] - 0s 430ms/step\n",
      "151 [D loss: 0.06700304709374905, acc.: 98.4375] [G loss: 2.2479734420776367]\n",
      "1/1 [==============================] - 0s 492ms/step\n",
      "152 [D loss: 0.06806820631027222, acc.: 100.0] [G loss: 1.0750066041946411]\n",
      "1/1 [==============================] - 0s 408ms/step\n",
      "153 [D loss: 0.0469402689486742, acc.: 100.0] [G loss: 1.4071077108383179]\n",
      "1/1 [==============================] - 1s 511ms/step\n",
      "154 [D loss: 0.06192974001169205, acc.: 98.4375] [G loss: 1.8209245204925537]\n",
      "1/1 [==============================] - 1s 552ms/step\n",
      "155 [D loss: 0.037006836384534836, acc.: 100.0] [G loss: 1.9496245384216309]\n",
      "1/1 [==============================] - 1s 551ms/step\n",
      "156 [D loss: 0.060063788667321205, acc.: 100.0] [G loss: 1.1410818099975586]\n",
      "1/1 [==============================] - 0s 380ms/step\n",
      "157 [D loss: 0.09917706064879894, acc.: 98.4375] [G loss: 3.395373821258545]\n",
      "1/1 [==============================] - 1s 522ms/step\n",
      "158 [D loss: 0.11441101017408073, acc.: 95.3125] [G loss: 1.3958219289779663]\n",
      "1/1 [==============================] - 0s 466ms/step\n",
      "159 [D loss: 0.017666793428361416, acc.: 100.0] [G loss: 1.326794147491455]\n",
      "1/1 [==============================] - 1s 744ms/step\n",
      "160 [D loss: 0.06443504802882671, acc.: 98.4375] [G loss: 1.9968054294586182]\n",
      "1/1 [==============================] - 0s 418ms/step\n",
      "161 [D loss: 0.02188650658354163, acc.: 100.0] [G loss: 2.1848855018615723]\n",
      "1/1 [==============================] - 0s 481ms/step\n",
      "162 [D loss: 0.04673928767442703, acc.: 100.0] [G loss: 1.2098511457443237]\n",
      "1/1 [==============================] - 0s 431ms/step\n",
      "163 [D loss: 0.07566003780812025, acc.: 98.4375] [G loss: 2.252845287322998]\n",
      "1/1 [==============================] - 1s 500ms/step\n",
      "164 [D loss: 0.033476094249635935, acc.: 98.4375] [G loss: 2.1036252975463867]\n",
      "1/1 [==============================] - 1s 517ms/step\n",
      "165 [D loss: 0.07748783379793167, acc.: 100.0] [G loss: 1.1356885433197021]\n",
      "1/1 [==============================] - 0s 417ms/step\n",
      "166 [D loss: 0.15139648132026196, acc.: 95.3125] [G loss: 2.2718358039855957]\n",
      "1/1 [==============================] - 0s 444ms/step\n",
      "167 [D loss: 0.039801407139748335, acc.: 100.0] [G loss: 1.652578592300415]\n",
      "1/1 [==============================] - 0s 406ms/step\n",
      "168 [D loss: 0.08839062601327896, acc.: 98.4375] [G loss: 2.1610488891601562]\n",
      "1/1 [==============================] - 0s 462ms/step\n",
      "169 [D loss: 0.06820664927363396, acc.: 100.0] [G loss: 1.165597915649414]\n",
      "1/1 [==============================] - 0s 407ms/step\n",
      "170 [D loss: 0.045244017615914345, acc.: 100.0] [G loss: 1.8455004692077637]\n",
      "1/1 [==============================] - 0s 396ms/step\n",
      "171 [D loss: 0.09847505390644073, acc.: 95.3125] [G loss: 3.186171770095825]\n",
      "1/1 [==============================] - 0s 490ms/step\n",
      "172 [D loss: 0.10144690179731697, acc.: 96.875] [G loss: 1.2800172567367554]\n",
      "1/1 [==============================] - 0s 493ms/step\n",
      "173 [D loss: 0.015841659624129534, acc.: 100.0] [G loss: 0.7960762977600098]\n",
      "1/1 [==============================] - 1s 627ms/step\n",
      "174 [D loss: 0.04756203759461641, acc.: 98.4375] [G loss: 1.315036416053772]\n",
      "1/1 [==============================] - 1s 505ms/step\n",
      "175 [D loss: 0.05002720095217228, acc.: 100.0] [G loss: 2.5991272926330566]\n",
      "1/1 [==============================] - 0s 412ms/step\n",
      "176 [D loss: 0.026594090973958373, acc.: 100.0] [G loss: 2.0716800689697266]\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "177 [D loss: 0.08615163061767817, acc.: 98.4375] [G loss: 0.5907315015792847]\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "178 [D loss: 0.12224897928535938, acc.: 95.3125] [G loss: 1.422166347503662]\n",
      "1/1 [==============================] - 1s 506ms/step\n",
      "179 [D loss: 0.029230706160888076, acc.: 98.4375] [G loss: 2.388127326965332]\n",
      "1/1 [==============================] - 0s 412ms/step\n",
      "180 [D loss: 0.043943521566689014, acc.: 100.0] [G loss: 1.2123481035232544]\n",
      "1/1 [==============================] - 0s 465ms/step\n",
      "181 [D loss: 0.05186441168189049, acc.: 100.0] [G loss: 1.5576354265213013]\n",
      "1/1 [==============================] - 0s 461ms/step\n",
      "182 [D loss: 0.1023824866861105, acc.: 95.3125] [G loss: 2.18959903717041]\n",
      "1/1 [==============================] - 0s 428ms/step\n",
      "183 [D loss: 0.09337297827005386, acc.: 96.875] [G loss: 1.504457712173462]\n",
      "1/1 [==============================] - 0s 406ms/step\n",
      "184 [D loss: 0.03425516188144684, acc.: 100.0] [G loss: 1.2887423038482666]\n",
      "1/1 [==============================] - 0s 483ms/step\n",
      "185 [D loss: 0.05209357663989067, acc.: 100.0] [G loss: 1.2801449298858643]\n",
      "1/1 [==============================] - 0s 450ms/step\n",
      "186 [D loss: 0.012004752177745104, acc.: 100.0] [G loss: 1.3662649393081665]\n",
      "1/1 [==============================] - 0s 490ms/step\n",
      "187 [D loss: 0.027230664156377316, acc.: 100.0] [G loss: 1.5433169603347778]\n",
      "1/1 [==============================] - 0s 350ms/step\n",
      "188 [D loss: 0.1911168396472931, acc.: 93.75] [G loss: 1.3058865070343018]\n",
      "1/1 [==============================] - 1s 565ms/step\n",
      "189 [D loss: 0.023675560485571623, acc.: 100.0] [G loss: 1.4062386751174927]\n",
      "1/1 [==============================] - 0s 378ms/step\n",
      "190 [D loss: 0.0486249141395092, acc.: 100.0] [G loss: 0.9544610977172852]\n",
      "1/1 [==============================] - 0s 469ms/step\n",
      "191 [D loss: 0.056081429589539766, acc.: 98.4375] [G loss: 1.5115489959716797]\n",
      "1/1 [==============================] - 0s 454ms/step\n",
      "192 [D loss: 0.04972656071186066, acc.: 98.4375] [G loss: 1.3356024026870728]\n",
      "1/1 [==============================] - 0s 445ms/step\n",
      "193 [D loss: 0.044602871872484684, acc.: 100.0] [G loss: 1.1237292289733887]\n",
      "1/1 [==============================] - 0s 413ms/step\n",
      "194 [D loss: 0.053538715466856956, acc.: 96.875] [G loss: 1.1909111738204956]\n",
      "1/1 [==============================] - 0s 414ms/step\n",
      "195 [D loss: 0.013069506734609604, acc.: 100.0] [G loss: 1.3983104228973389]\n",
      "1/1 [==============================] - 1s 570ms/step\n",
      "196 [D loss: 0.05681246519088745, acc.: 100.0] [G loss: 1.4346890449523926]\n",
      "1/1 [==============================] - 0s 392ms/step\n",
      "197 [D loss: 0.04805193841457367, acc.: 98.4375] [G loss: 0.8144530057907104]\n",
      "1/1 [==============================] - 0s 461ms/step\n",
      "198 [D loss: 0.04450832773000002, acc.: 98.4375] [G loss: 1.1517726182937622]\n",
      "1/1 [==============================] - 1s 544ms/step\n",
      "199 [D loss: 0.013276829849928617, acc.: 100.0] [G loss: 1.150235652923584]\n",
      "1/1 [==============================] - 0s 430ms/step\n",
      "200 [D loss: 0.03898956626653671, acc.: 100.0] [G loss: 0.8779796361923218]\n",
      "1/1 [==============================] - 0s 323ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "201 [D loss: 0.011497796978801489, acc.: 100.0] [G loss: 0.9189179539680481]\n",
      "1/1 [==============================] - 0s 477ms/step\n",
      "202 [D loss: 0.023147238418459892, acc.: 100.0] [G loss: 1.163696050643921]\n",
      "1/1 [==============================] - 0s 444ms/step\n",
      "203 [D loss: 0.02796654123812914, acc.: 100.0] [G loss: 1.4467389583587646]\n",
      "1/1 [==============================] - 0s 390ms/step\n",
      "204 [D loss: 0.05216272175312042, acc.: 98.4375] [G loss: 1.1341779232025146]\n",
      "1/1 [==============================] - 1s 512ms/step\n",
      "205 [D loss: 0.047561805695295334, acc.: 100.0] [G loss: 1.2677761316299438]\n",
      "1/1 [==============================] - 1s 510ms/step\n",
      "206 [D loss: 0.027072320692241192, acc.: 100.0] [G loss: 1.1912963390350342]\n",
      "1/1 [==============================] - 1s 649ms/step\n",
      "207 [D loss: 0.02356903674080968, acc.: 100.0] [G loss: 1.440889835357666]\n",
      "1/1 [==============================] - 0s 440ms/step\n",
      "208 [D loss: 0.034696257673203945, acc.: 100.0] [G loss: 1.0818493366241455]\n",
      "1/1 [==============================] - 0s 403ms/step\n",
      "209 [D loss: 0.012476056348532438, acc.: 100.0] [G loss: 0.9728289842605591]\n",
      "1/1 [==============================] - 0s 422ms/step\n",
      "210 [D loss: 0.029259106144309044, acc.: 100.0] [G loss: 1.0689496994018555]\n",
      "1/1 [==============================] - 0s 421ms/step\n",
      "211 [D loss: 0.030727887526154518, acc.: 100.0] [G loss: 1.5297908782958984]\n",
      "1/1 [==============================] - 0s 420ms/step\n",
      "212 [D loss: 0.01817670650780201, acc.: 100.0] [G loss: 1.0254216194152832]\n",
      "1/1 [==============================] - 0s 487ms/step\n",
      "213 [D loss: 0.005676653003320098, acc.: 100.0] [G loss: 1.046465277671814]\n",
      "1/1 [==============================] - 0s 481ms/step\n",
      "214 [D loss: 0.01799250952899456, acc.: 100.0] [G loss: 0.9084019660949707]\n",
      "1/1 [==============================] - 0s 469ms/step\n",
      "215 [D loss: 0.026410788297653198, acc.: 100.0] [G loss: 0.9075101017951965]\n",
      "1/1 [==============================] - 1s 510ms/step\n",
      "216 [D loss: 0.03298622090369463, acc.: 98.4375] [G loss: 1.6505610942840576]\n",
      "1/1 [==============================] - 0s 458ms/step\n",
      "217 [D loss: 0.024658628273755312, acc.: 100.0] [G loss: 1.1636576652526855]\n",
      "1/1 [==============================] - 1s 517ms/step\n",
      "218 [D loss: 0.027107452508062124, acc.: 100.0] [G loss: 0.972421407699585]\n",
      "1/1 [==============================] - 0s 443ms/step\n",
      "219 [D loss: 0.017919393256306648, acc.: 100.0] [G loss: 0.6117638349533081]\n",
      "1/1 [==============================] - 1s 672ms/step\n",
      "220 [D loss: 0.04055067524313927, acc.: 100.0] [G loss: 1.282525658607483]\n",
      "1/1 [==============================] - 0s 444ms/step\n",
      "221 [D loss: 0.047189148142933846, acc.: 100.0] [G loss: 0.9017859697341919]\n",
      "1/1 [==============================] - 1s 544ms/step\n",
      "222 [D loss: 0.05141758732497692, acc.: 100.0] [G loss: 1.3341028690338135]\n",
      "1/1 [==============================] - 0s 466ms/step\n",
      "223 [D loss: 0.03253976535052061, acc.: 100.0] [G loss: 0.7842648029327393]\n",
      "1/1 [==============================] - 0s 410ms/step\n",
      "224 [D loss: 0.039218002930283546, acc.: 98.4375] [G loss: 0.9359365701675415]\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "225 [D loss: 0.01631137973163277, acc.: 100.0] [G loss: 1.0414693355560303]\n",
      "1/1 [==============================] - 1s 533ms/step\n",
      "226 [D loss: 0.024615959264338017, acc.: 100.0] [G loss: 1.263484239578247]\n",
      "1/1 [==============================] - 0s 402ms/step\n",
      "227 [D loss: 0.02177380584180355, acc.: 100.0] [G loss: 1.1657118797302246]\n",
      "1/1 [==============================] - 1s 510ms/step\n",
      "228 [D loss: 0.012404451379552484, acc.: 100.0] [G loss: 1.1072511672973633]\n",
      "1/1 [==============================] - 0s 496ms/step\n",
      "229 [D loss: 0.017927338369190693, acc.: 100.0] [G loss: 1.1003636121749878]\n",
      "1/1 [==============================] - 0s 493ms/step\n",
      "230 [D loss: 0.018983555026352406, acc.: 100.0] [G loss: 1.1681592464447021]\n",
      "1/1 [==============================] - 1s 559ms/step\n",
      "231 [D loss: 0.01632194174453616, acc.: 100.0] [G loss: 1.3228919506072998]\n",
      "1/1 [==============================] - 0s 461ms/step\n",
      "232 [D loss: 0.021570541895926, acc.: 100.0] [G loss: 0.738911509513855]\n",
      "1/1 [==============================] - 0s 389ms/step\n",
      "233 [D loss: 0.027478800155222416, acc.: 100.0] [G loss: 0.9174063205718994]\n",
      "1/1 [==============================] - 0s 497ms/step\n",
      "234 [D loss: 0.007916375994682312, acc.: 100.0] [G loss: 1.3650068044662476]\n",
      "1/1 [==============================] - 1s 523ms/step\n",
      "235 [D loss: 0.016233636299148202, acc.: 100.0] [G loss: 1.147722601890564]\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "236 [D loss: 0.02477256115525961, acc.: 100.0] [G loss: 1.2083921432495117]\n",
      "1/1 [==============================] - 0s 478ms/step\n",
      "237 [D loss: 0.01930117141455412, acc.: 100.0] [G loss: 0.937781572341919]\n",
      "1/1 [==============================] - 0s 487ms/step\n",
      "238 [D loss: 0.015986742917448282, acc.: 100.0] [G loss: 0.8855916261672974]\n",
      "1/1 [==============================] - 1s 560ms/step\n",
      "239 [D loss: 0.010031787678599358, acc.: 100.0] [G loss: 1.1415711641311646]\n",
      "1/1 [==============================] - 0s 459ms/step\n",
      "240 [D loss: 0.008534997701644897, acc.: 100.0] [G loss: 0.904271125793457]\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "241 [D loss: 0.010969706811010838, acc.: 100.0] [G loss: 0.940596342086792]\n",
      "1/1 [==============================] - 0s 410ms/step\n",
      "242 [D loss: 0.023943817242980003, acc.: 100.0] [G loss: 1.3177928924560547]\n",
      "1/1 [==============================] - 0s 442ms/step\n",
      "243 [D loss: 0.016919264802709222, acc.: 100.0] [G loss: 1.0391252040863037]\n",
      "1/1 [==============================] - 0s 438ms/step\n",
      "244 [D loss: 0.035035086795687675, acc.: 100.0] [G loss: 0.7610752582550049]\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "245 [D loss: 0.02421730011701584, acc.: 100.0] [G loss: 0.8024924993515015]\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "246 [D loss: 0.01081802323460579, acc.: 100.0] [G loss: 1.1021595001220703]\n",
      "1/1 [==============================] - 0s 398ms/step\n",
      "247 [D loss: 0.028838169761002064, acc.: 100.0] [G loss: 0.8359969258308411]\n",
      "1/1 [==============================] - 0s 434ms/step\n",
      "248 [D loss: 0.012578080873936415, acc.: 100.0] [G loss: 0.8923647403717041]\n",
      "1/1 [==============================] - 0s 409ms/step\n",
      "249 [D loss: 0.0033613245468586683, acc.: 100.0] [G loss: 0.6811615228652954]\n",
      "1/1 [==============================] - 0s 470ms/step\n",
      "250 [D loss: 0.010566516779363155, acc.: 100.0] [G loss: 0.9188644289970398]\n",
      "1/1 [==============================] - 0s 467ms/step\n",
      "251 [D loss: 0.03749176859855652, acc.: 98.4375] [G loss: 0.9186801910400391]\n",
      "1/1 [==============================] - 1s 511ms/step\n",
      "252 [D loss: 0.009821529500186443, acc.: 100.0] [G loss: 1.138326644897461]\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "253 [D loss: 0.016967308707535267, acc.: 100.0] [G loss: 1.1606098413467407]\n",
      "1/1 [==============================] - 0s 393ms/step\n",
      "254 [D loss: 0.009006149601191282, acc.: 100.0] [G loss: 0.6976397037506104]\n",
      "1/1 [==============================] - 0s 429ms/step\n",
      "255 [D loss: 0.02434039767831564, acc.: 100.0] [G loss: 0.8055272102355957]\n",
      "1/1 [==============================] - 0s 453ms/step\n",
      "256 [D loss: 0.011873605195432901, acc.: 100.0] [G loss: 1.1049959659576416]\n",
      "1/1 [==============================] - 0s 419ms/step\n",
      "257 [D loss: 0.01585020893253386, acc.: 100.0] [G loss: 0.8839354515075684]\n",
      "1/1 [==============================] - 0s 445ms/step\n",
      "258 [D loss: 0.011606171727180481, acc.: 100.0] [G loss: 0.8070509433746338]\n",
      "1/1 [==============================] - 0s 485ms/step\n",
      "259 [D loss: 0.004197864327579737, acc.: 100.0] [G loss: 0.7148028612136841]\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "260 [D loss: 0.021427768282592297, acc.: 100.0] [G loss: 0.7845133543014526]\n",
      "1/1 [==============================] - 0s 465ms/step\n",
      "261 [D loss: 0.00934755802154541, acc.: 100.0] [G loss: 0.9494791626930237]\n",
      "1/1 [==============================] - 0s 438ms/step\n",
      "262 [D loss: 0.008549354737624526, acc.: 100.0] [G loss: 0.8317273259162903]\n",
      "1/1 [==============================] - 1s 508ms/step\n",
      "263 [D loss: 0.06563405320048332, acc.: 96.875] [G loss: 1.005408525466919]\n",
      "1/1 [==============================] - 0s 411ms/step\n",
      "264 [D loss: 0.008807586273178458, acc.: 100.0] [G loss: 0.97881680727005]\n",
      "1/1 [==============================] - 0s 412ms/step\n",
      "265 [D loss: 0.010655353777110577, acc.: 100.0] [G loss: 0.9236586093902588]\n",
      "1/1 [==============================] - 0s 439ms/step\n",
      "266 [D loss: 0.009862459264695644, acc.: 100.0] [G loss: 0.934594452381134]\n",
      "1/1 [==============================] - 0s 422ms/step\n",
      "267 [D loss: 0.007453311234712601, acc.: 100.0] [G loss: 0.9243295192718506]\n",
      "1/1 [==============================] - 1s 510ms/step\n",
      "268 [D loss: 0.008431598544120789, acc.: 100.0] [G loss: 0.7076031565666199]\n",
      "1/1 [==============================] - 1s 507ms/step\n",
      "269 [D loss: 0.011409810744225979, acc.: 100.0] [G loss: 0.6640201807022095]\n",
      "1/1 [==============================] - 0s 458ms/step\n",
      "270 [D loss: 0.02739541605114937, acc.: 98.4375] [G loss: 0.6755954027175903]\n",
      "1/1 [==============================] - 0s 453ms/step\n",
      "271 [D loss: 0.009172578807920218, acc.: 100.0] [G loss: 0.9766111969947815]\n",
      "1/1 [==============================] - 0s 457ms/step\n",
      "272 [D loss: 0.016121085733175278, acc.: 100.0] [G loss: 0.7518230676651001]\n",
      "1/1 [==============================] - 1s 565ms/step\n",
      "273 [D loss: 0.010580718517303467, acc.: 100.0] [G loss: 0.6240286827087402]\n",
      "1/1 [==============================] - 0s 485ms/step\n",
      "274 [D loss: 0.020557384006679058, acc.: 100.0] [G loss: 0.5598960518836975]\n",
      "1/1 [==============================] - 0s 490ms/step\n",
      "275 [D loss: 0.012260859366506338, acc.: 100.0] [G loss: 1.1629045009613037]\n",
      "1/1 [==============================] - 0s 447ms/step\n",
      "276 [D loss: 0.033194066025316715, acc.: 100.0] [G loss: 1.0952818393707275]\n",
      "1/1 [==============================] - 0s 450ms/step\n",
      "277 [D loss: 0.006488016340881586, acc.: 100.0] [G loss: 0.8832442164421082]\n",
      "1/1 [==============================] - 0s 445ms/step\n",
      "278 [D loss: 0.015531966462731361, acc.: 100.0] [G loss: 0.7423019409179688]\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "279 [D loss: 0.011968911625444889, acc.: 100.0] [G loss: 0.7831155061721802]\n",
      "1/1 [==============================] - 0s 464ms/step\n",
      "280 [D loss: 0.008671658113598824, acc.: 100.0] [G loss: 0.8605692982673645]\n",
      "1/1 [==============================] - 0s 410ms/step\n",
      "281 [D loss: 0.01597703341394663, acc.: 100.0] [G loss: 0.8970990777015686]\n",
      "1/1 [==============================] - 0s 482ms/step\n",
      "282 [D loss: 0.008756996132433414, acc.: 100.0] [G loss: 0.7617225646972656]\n",
      "1/1 [==============================] - 0s 489ms/step\n",
      "283 [D loss: 0.005170412361621857, acc.: 100.0] [G loss: 0.7286720275878906]\n",
      "1/1 [==============================] - 0s 492ms/step\n",
      "284 [D loss: 0.005739080370403826, acc.: 100.0] [G loss: 0.8801066875457764]\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "285 [D loss: 0.012389148585498333, acc.: 100.0] [G loss: 1.0308525562286377]\n",
      "1/1 [==============================] - 0s 463ms/step\n",
      "286 [D loss: 0.00893251970410347, acc.: 100.0] [G loss: 0.6670272946357727]\n",
      "1/1 [==============================] - 0s 440ms/step\n",
      "287 [D loss: 0.027005420997738838, acc.: 100.0] [G loss: 1.0789031982421875]\n",
      "1/1 [==============================] - 0s 453ms/step\n",
      "288 [D loss: 0.007958319270983338, acc.: 100.0] [G loss: 1.1389379501342773]\n",
      "1/1 [==============================] - 0s 470ms/step\n",
      "289 [D loss: 0.024547857232391834, acc.: 100.0] [G loss: 0.8397716879844666]\n",
      "1/1 [==============================] - 0s 475ms/step\n",
      "290 [D loss: 0.012268425896763802, acc.: 100.0] [G loss: 0.5596753358840942]\n",
      "1/1 [==============================] - 1s 525ms/step\n",
      "291 [D loss: 0.01115765213035047, acc.: 100.0] [G loss: 0.7555151581764221]\n",
      "1/1 [==============================] - 0s 478ms/step\n",
      "292 [D loss: 0.013969366205856204, acc.: 100.0] [G loss: 1.3699795007705688]\n",
      "1/1 [==============================] - 0s 454ms/step\n",
      "293 [D loss: 0.017871415242552757, acc.: 100.0] [G loss: 0.8446106910705566]\n",
      "1/1 [==============================] - 0s 446ms/step\n",
      "294 [D loss: 0.009503612294793129, acc.: 100.0] [G loss: 0.734459400177002]\n",
      "1/1 [==============================] - 0s 479ms/step\n",
      "295 [D loss: 0.02747412398457527, acc.: 100.0] [G loss: 0.8052124977111816]\n",
      "1/1 [==============================] - 0s 487ms/step\n",
      "296 [D loss: 0.0064810835756361485, acc.: 100.0] [G loss: 1.1579669713974]\n",
      "1/1 [==============================] - 0s 462ms/step\n",
      "297 [D loss: 0.0036149852676317096, acc.: 100.0] [G loss: 1.1316531896591187]\n",
      "1/1 [==============================] - 0s 459ms/step\n",
      "298 [D loss: 0.017620478989556432, acc.: 100.0] [G loss: 0.7256360650062561]\n",
      "1/1 [==============================] - 0s 440ms/step\n",
      "299 [D loss: 0.016741194296628237, acc.: 100.0] [G loss: 0.8349282145500183]\n",
      "1/1 [==============================] - 0s 418ms/step\n",
      "300 [D loss: 0.0050233223009854555, acc.: 100.0] [G loss: 0.6708364486694336]\n",
      "1/1 [==============================] - 0s 377ms/step\n",
      "1/1 [==============================] - 1s 613ms/step\n",
      "301 [D loss: 0.006581137655302882, acc.: 100.0] [G loss: 1.0203920602798462]\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "302 [D loss: 0.006628933711908758, acc.: 100.0] [G loss: 0.7798092365264893]\n",
      "1/1 [==============================] - 0s 462ms/step\n",
      "303 [D loss: 0.005951327504590154, acc.: 100.0] [G loss: 0.8018440008163452]\n",
      "1/1 [==============================] - 0s 480ms/step\n",
      "304 [D loss: 0.009480365552008152, acc.: 100.0] [G loss: 0.8472733497619629]\n",
      "1/1 [==============================] - 0s 448ms/step\n",
      "305 [D loss: 0.008139117620885372, acc.: 100.0] [G loss: 0.829007089138031]\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "306 [D loss: 0.009865762200206518, acc.: 100.0] [G loss: 0.7952980995178223]\n",
      "1/1 [==============================] - 0s 447ms/step\n",
      "307 [D loss: 0.01573015726171434, acc.: 100.0] [G loss: 0.6215406060218811]\n",
      "1/1 [==============================] - 0s 440ms/step\n",
      "308 [D loss: 0.01482538366690278, acc.: 100.0] [G loss: 0.7965246438980103]\n",
      "1/1 [==============================] - 0s 444ms/step\n",
      "309 [D loss: 0.01296395726967603, acc.: 100.0] [G loss: 0.7128047943115234]\n",
      "1/1 [==============================] - 0s 425ms/step\n",
      "310 [D loss: 0.0089842330198735, acc.: 100.0] [G loss: 0.7466549277305603]\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "311 [D loss: 0.004962410079315305, acc.: 100.0] [G loss: 0.9338545799255371]\n",
      "1/1 [==============================] - 0s 471ms/step\n",
      "312 [D loss: 0.014402415603399277, acc.: 100.0] [G loss: 0.8857787251472473]\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "313 [D loss: 0.005390919977799058, acc.: 100.0] [G loss: 0.7428387403488159]\n",
      "1/1 [==============================] - 1s 575ms/step\n",
      "314 [D loss: 0.019494473934173584, acc.: 100.0] [G loss: 0.8790076971054077]\n",
      "1/1 [==============================] - 0s 470ms/step\n",
      "315 [D loss: 0.0041052274173125625, acc.: 100.0] [G loss: 1.0964375734329224]\n",
      "1/1 [==============================] - 0s 485ms/step\n",
      "316 [D loss: 0.006730168825015426, acc.: 100.0] [G loss: 0.7733691930770874]\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "317 [D loss: 0.006896295351907611, acc.: 100.0] [G loss: 0.8651168942451477]\n",
      "1/1 [==============================] - 0s 458ms/step\n",
      "318 [D loss: 0.010494498070329428, acc.: 100.0] [G loss: 0.8184897303581238]\n",
      "1/1 [==============================] - 0s 450ms/step\n",
      "319 [D loss: 0.017958268523216248, acc.: 100.0] [G loss: 0.7877138257026672]\n",
      "1/1 [==============================] - 0s 475ms/step\n",
      "320 [D loss: 0.01717415265738964, acc.: 100.0] [G loss: 0.699700117111206]\n",
      "1/1 [==============================] - 0s 464ms/step\n",
      "321 [D loss: 0.014557013288140297, acc.: 100.0] [G loss: 0.8206768035888672]\n",
      "1/1 [==============================] - 0s 440ms/step\n",
      "322 [D loss: 0.011607063934206963, acc.: 100.0] [G loss: 0.8075792193412781]\n",
      "1/1 [==============================] - 0s 450ms/step\n",
      "323 [D loss: 0.012505876598879695, acc.: 100.0] [G loss: 0.7096012830734253]\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "324 [D loss: 0.006630209973081946, acc.: 100.0] [G loss: 0.568755030632019]\n",
      "1/1 [==============================] - 0s 439ms/step\n",
      "325 [D loss: 0.008018475025892258, acc.: 100.0] [G loss: 0.530418872833252]\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "326 [D loss: 0.008096091914921999, acc.: 100.0] [G loss: 0.8431088924407959]\n",
      "1/1 [==============================] - 0s 477ms/step\n",
      "327 [D loss: 0.008437516866251826, acc.: 100.0] [G loss: 1.0753638744354248]\n",
      "1/1 [==============================] - 1s 531ms/step\n",
      "328 [D loss: 0.011244622990489006, acc.: 100.0] [G loss: 0.7469630837440491]\n",
      "1/1 [==============================] - 0s 406ms/step\n",
      "329 [D loss: 0.009812873089686036, acc.: 100.0] [G loss: 0.7217199802398682]\n",
      "1/1 [==============================] - 0s 427ms/step\n",
      "330 [D loss: 0.005899765994399786, acc.: 100.0] [G loss: 0.6006476879119873]\n",
      "1/1 [==============================] - 0s 419ms/step\n",
      "331 [D loss: 0.004356688354164362, acc.: 100.0] [G loss: 0.7833396196365356]\n",
      "1/1 [==============================] - 0s 420ms/step\n",
      "332 [D loss: 0.009946544421836734, acc.: 100.0] [G loss: 0.9001673460006714]\n",
      "1/1 [==============================] - 0s 492ms/step\n",
      "333 [D loss: 0.00807431386783719, acc.: 100.0] [G loss: 0.5404090285301208]\n",
      "1/1 [==============================] - 1s 590ms/step\n",
      "334 [D loss: 0.01750699826516211, acc.: 100.0] [G loss: 0.9292516112327576]\n",
      "1/1 [==============================] - 0s 443ms/step\n",
      "335 [D loss: 0.010205404134467244, acc.: 100.0] [G loss: 1.1973652839660645]\n",
      "1/1 [==============================] - 0s 407ms/step\n",
      "336 [D loss: 0.01520191878080368, acc.: 100.0] [G loss: 0.7897672653198242]\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "337 [D loss: 0.012558425310999155, acc.: 100.0] [G loss: 0.9320308566093445]\n",
      "1/1 [==============================] - 0s 448ms/step\n",
      "338 [D loss: 0.019425437785685062, acc.: 100.0] [G loss: 1.1524617671966553]\n",
      "1/1 [==============================] - 1s 510ms/step\n",
      "339 [D loss: 0.008564194897189736, acc.: 100.0] [G loss: 0.807121753692627]\n",
      "1/1 [==============================] - 0s 475ms/step\n",
      "340 [D loss: 0.0058210005518049, acc.: 100.0] [G loss: 0.7994487285614014]\n",
      "1/1 [==============================] - 0s 499ms/step\n",
      "341 [D loss: 0.003077409230172634, acc.: 100.0] [G loss: 1.11226224899292]\n",
      "1/1 [==============================] - 0s 451ms/step\n",
      "342 [D loss: 0.010670766700059175, acc.: 100.0] [G loss: 0.8546186685562134]\n",
      "1/1 [==============================] - 1s 518ms/step\n",
      "343 [D loss: 0.005899404641240835, acc.: 100.0] [G loss: 0.8651018142700195]\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "344 [D loss: 0.008929409086704254, acc.: 100.0] [G loss: 0.7306044101715088]\n",
      "1/1 [==============================] - 0s 462ms/step\n",
      "345 [D loss: 0.005089715356007218, acc.: 100.0] [G loss: 0.9307300448417664]\n",
      "1/1 [==============================] - 0s 499ms/step\n",
      "346 [D loss: 0.00821823813021183, acc.: 100.0] [G loss: 0.8033457398414612]\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "347 [D loss: 0.004898288985714316, acc.: 100.0] [G loss: 0.9077596664428711]\n",
      "1/1 [==============================] - 0s 441ms/step\n",
      "348 [D loss: 0.07462349347770214, acc.: 98.4375] [G loss: 3.3724827766418457]\n",
      "1/1 [==============================] - 0s 498ms/step\n",
      "349 [D loss: 4.448249191045761, acc.: 31.25] [G loss: 14.016046524047852]\n",
      "1/1 [==============================] - 0s 440ms/step\n",
      "350 [D loss: 4.031435770879284, acc.: 50.0] [G loss: 4.3946123123168945]\n",
      "1/1 [==============================] - 0s 444ms/step\n",
      "351 [D loss: 0.0069533475470962, acc.: 100.0] [G loss: 1.4717965126037598]\n",
      "1/1 [==============================] - 0s 447ms/step\n",
      "352 [D loss: 1.803302117579733, acc.: 54.6875] [G loss: 10.817242622375488]\n",
      "1/1 [==============================] - 0s 455ms/step\n",
      "353 [D loss: 0.48192540324604494, acc.: 84.375] [G loss: 4.6928253173828125]\n",
      "1/1 [==============================] - 0s 470ms/step\n",
      "354 [D loss: 0.6648224845848745, acc.: 76.5625] [G loss: 0.24555015563964844]\n",
      "1/1 [==============================] - 0s 477ms/step\n",
      "355 [D loss: 0.023909214476589113, acc.: 100.0] [G loss: 0.1671086847782135]\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "356 [D loss: 0.020094797109777573, acc.: 100.0] [G loss: 0.3879157304763794]\n",
      "1/1 [==============================] - 0s 479ms/step\n",
      "357 [D loss: 0.01562135963467881, acc.: 100.0] [G loss: 0.5008753538131714]\n",
      "1/1 [==============================] - 0s 495ms/step\n",
      "358 [D loss: 0.07528362673474476, acc.: 96.875] [G loss: 1.542388916015625]\n",
      "1/1 [==============================] - 0s 491ms/step\n",
      "359 [D loss: 0.11928806730429642, acc.: 95.3125] [G loss: 3.567869186401367]\n",
      "1/1 [==============================] - 0s 464ms/step\n",
      "360 [D loss: 0.005639887584038661, acc.: 100.0] [G loss: 3.818011522293091]\n",
      "1/1 [==============================] - 0s 409ms/step\n",
      "361 [D loss: 0.020170148178976888, acc.: 100.0] [G loss: 2.7788171768188477]\n",
      "1/1 [==============================] - 1s 519ms/step\n",
      "362 [D loss: 0.035989510448416695, acc.: 98.4375] [G loss: 1.716525912284851]\n",
      "1/1 [==============================] - 0s 480ms/step\n",
      "363 [D loss: 0.005181251748581417, acc.: 100.0] [G loss: 0.9391668438911438]\n",
      "1/1 [==============================] - 0s 486ms/step\n",
      "364 [D loss: 0.005122218746691942, acc.: 100.0] [G loss: 0.73659747838974]\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "365 [D loss: 0.022523467428982258, acc.: 100.0] [G loss: 0.46783721446990967]\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "366 [D loss: 0.008480984717607498, acc.: 100.0] [G loss: 0.5477990508079529]\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "367 [D loss: 0.017825434217229486, acc.: 98.4375] [G loss: 0.8409380912780762]\n",
      "1/1 [==============================] - 0s 480ms/step\n",
      "368 [D loss: 0.0027844877913594246, acc.: 100.0] [G loss: 0.7418889403343201]\n",
      "1/1 [==============================] - 1s 504ms/step\n",
      "369 [D loss: 0.012606751755811274, acc.: 100.0] [G loss: 0.6189839243888855]\n",
      "1/1 [==============================] - 0s 448ms/step\n",
      "370 [D loss: 0.008649897063151002, acc.: 100.0] [G loss: 0.5468274354934692]\n",
      "1/1 [==============================] - 1s 540ms/step\n",
      "371 [D loss: 0.00926143559627235, acc.: 100.0] [G loss: 0.30736538767814636]\n",
      "1/1 [==============================] - 1s 530ms/step\n",
      "372 [D loss: 0.030756119405850768, acc.: 98.4375] [G loss: 0.6308407783508301]\n",
      "1/1 [==============================] - 1s 515ms/step\n",
      "373 [D loss: 0.036282315850257874, acc.: 98.4375] [G loss: 0.648840069770813]\n",
      "1/1 [==============================] - 0s 467ms/step\n",
      "374 [D loss: 0.0044379098690114915, acc.: 100.0] [G loss: 0.5556439161300659]\n",
      "1/1 [==============================] - 0s 297ms/step\n",
      "375 [D loss: 0.002249440527521074, acc.: 100.0] [G loss: 0.4304627776145935]\n",
      "1/1 [==============================] - 0s 307ms/step\n",
      "376 [D loss: 0.003988881129771471, acc.: 100.0] [G loss: 0.45600491762161255]\n",
      "1/1 [==============================] - 0s 317ms/step\n",
      "377 [D loss: 0.00585803622379899, acc.: 100.0] [G loss: 0.46221116185188293]\n",
      "1/1 [==============================] - 0s 323ms/step\n",
      "378 [D loss: 0.010774986119940877, acc.: 100.0] [G loss: 0.4069107174873352]\n",
      "1/1 [==============================] - 0s 325ms/step\n",
      "379 [D loss: 0.009898942895233631, acc.: 100.0] [G loss: 0.22764401137828827]\n",
      "1/1 [==============================] - 0s 322ms/step\n",
      "380 [D loss: 0.03223938401788473, acc.: 98.4375] [G loss: 0.5285564661026001]\n",
      "1/1 [==============================] - 0s 379ms/step\n",
      "381 [D loss: 0.003965239622630179, acc.: 100.0] [G loss: 0.5093585252761841]\n",
      "1/1 [==============================] - 0s 361ms/step\n",
      "382 [D loss: 0.008149751287419349, acc.: 100.0] [G loss: 0.5390571355819702]\n",
      "1/1 [==============================] - 0s 359ms/step\n",
      "383 [D loss: 0.002282465808093548, acc.: 100.0] [G loss: 0.49833202362060547]\n",
      "1/1 [==============================] - 0s 350ms/step\n",
      "384 [D loss: 0.0030127745121717453, acc.: 100.0] [G loss: 0.4195955991744995]\n",
      "1/1 [==============================] - 0s 317ms/step\n",
      "385 [D loss: 0.0026877698255702853, acc.: 100.0] [G loss: 0.5239627361297607]\n",
      "1/1 [==============================] - 0s 342ms/step\n",
      "386 [D loss: 0.013920018449425697, acc.: 100.0] [G loss: 0.4236412048339844]\n",
      "1/1 [==============================] - 0s 316ms/step\n",
      "387 [D loss: 0.007232873234897852, acc.: 100.0] [G loss: 0.3452555537223816]\n",
      "1/1 [==============================] - 0s 367ms/step\n",
      "388 [D loss: 0.007911040738690645, acc.: 100.0] [G loss: 0.3613995313644409]\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "389 [D loss: 0.004728824133053422, acc.: 100.0] [G loss: 0.23497633635997772]\n",
      "1/1 [==============================] - 0s 369ms/step\n",
      "390 [D loss: 0.0031147440895438194, acc.: 100.0] [G loss: 0.2883491814136505]\n",
      "1/1 [==============================] - 0s 313ms/step\n",
      "391 [D loss: 0.0089669618755579, acc.: 100.0] [G loss: 0.2744181454181671]\n",
      "1/1 [==============================] - 0s 383ms/step\n",
      "392 [D loss: 0.007613060995936394, acc.: 100.0] [G loss: 0.3042183816432953]\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "393 [D loss: 0.003362296149134636, acc.: 100.0] [G loss: 0.4648841619491577]\n",
      "1/1 [==============================] - 0s 376ms/step\n",
      "394 [D loss: 0.005035339156165719, acc.: 100.0] [G loss: 0.4528065621852875]\n",
      "1/1 [==============================] - 0s 317ms/step\n",
      "395 [D loss: 0.003774218959733844, acc.: 100.0] [G loss: 0.33806920051574707]\n",
      "1/1 [==============================] - 0s 326ms/step\n",
      "396 [D loss: 0.001656715030549094, acc.: 100.0] [G loss: 0.30016836524009705]\n",
      "1/1 [==============================] - 0s 356ms/step\n",
      "397 [D loss: 0.018491803435608745, acc.: 100.0] [G loss: 0.2722877562046051]\n",
      "1/1 [==============================] - 0s 378ms/step\n",
      "398 [D loss: 0.008614475838840008, acc.: 100.0] [G loss: 0.21331430971622467]\n",
      "1/1 [==============================] - 0s 317ms/step\n",
      "399 [D loss: 0.0032053927425295115, acc.: 100.0] [G loss: 0.13947176933288574]\n",
      "1/1 [==============================] - 0s 364ms/step\n",
      "400 [D loss: 0.009274184587411582, acc.: 100.0] [G loss: 0.18877430260181427]\n",
      "1/1 [==============================] - 0s 313ms/step\n",
      "1/1 [==============================] - 0s 362ms/step\n",
      "401 [D loss: 0.003352466272190213, acc.: 100.0] [G loss: 0.26747989654541016]\n",
      "1/1 [==============================] - 0s 422ms/step\n",
      "402 [D loss: 0.0026755474973469973, acc.: 100.0] [G loss: 0.27391088008880615]\n",
      "1/1 [==============================] - 0s 303ms/step\n",
      "403 [D loss: 0.0036199605092406273, acc.: 100.0] [G loss: 0.24444186687469482]\n",
      "1/1 [==============================] - 0s 377ms/step\n",
      "404 [D loss: 0.0032068208092823625, acc.: 100.0] [G loss: 0.1887233853340149]\n",
      "1/1 [==============================] - 0s 315ms/step\n",
      "405 [D loss: 0.004774564877152443, acc.: 100.0] [G loss: 0.2578433156013489]\n",
      "1/1 [==============================] - 0s 317ms/step\n",
      "406 [D loss: 0.013372599612921476, acc.: 100.0] [G loss: 0.31744569540023804]\n",
      "1/1 [==============================] - 0s 314ms/step\n",
      "407 [D loss: 0.006685081170871854, acc.: 100.0] [G loss: 0.20984116196632385]\n",
      "1/1 [==============================] - 0s 325ms/step\n",
      "408 [D loss: 0.009563776548020542, acc.: 100.0] [G loss: 0.32100868225097656]\n",
      "1/1 [==============================] - 0s 313ms/step\n",
      "409 [D loss: 0.001306054531596601, acc.: 100.0] [G loss: 0.26027244329452515]\n",
      "1/1 [==============================] - 0s 310ms/step\n",
      "410 [D loss: 0.002682001329958439, acc.: 100.0] [G loss: 0.25292181968688965]\n",
      "1/1 [==============================] - 0s 321ms/step\n",
      "411 [D loss: 0.007076590787619352, acc.: 100.0] [G loss: 0.26645827293395996]\n",
      "1/1 [==============================] - 0s 325ms/step\n",
      "412 [D loss: 0.002228297758847475, acc.: 100.0] [G loss: 0.15404853224754333]\n",
      "1/1 [==============================] - 0s 306ms/step\n",
      "413 [D loss: 0.003030420048162341, acc.: 100.0] [G loss: 0.21822793781757355]\n",
      "1/1 [==============================] - 0s 372ms/step\n",
      "414 [D loss: 0.004597585997544229, acc.: 100.0] [G loss: 0.1627557873725891]\n",
      "1/1 [==============================] - 0s 292ms/step\n",
      "415 [D loss: 0.005885038524866104, acc.: 100.0] [G loss: 0.21236984431743622]\n",
      "1/1 [==============================] - 0s 291ms/step\n",
      "416 [D loss: 0.00322585366666317, acc.: 100.0] [G loss: 0.21376028656959534]\n",
      "1/1 [==============================] - 0s 307ms/step\n",
      "417 [D loss: 0.002173687273170799, acc.: 100.0] [G loss: 0.18246936798095703]\n",
      "1/1 [==============================] - 0s 315ms/step\n",
      "418 [D loss: 0.0030773398466408253, acc.: 100.0] [G loss: 0.2095986306667328]\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "419 [D loss: 0.004424323153216392, acc.: 100.0] [G loss: 0.23615993559360504]\n",
      "1/1 [==============================] - 0s 320ms/step\n",
      "420 [D loss: 0.006397244520485401, acc.: 100.0] [G loss: 0.23846010863780975]\n",
      "1/1 [==============================] - 0s 331ms/step\n",
      "421 [D loss: 0.0020364909432828426, acc.: 100.0] [G loss: 0.14619603753089905]\n",
      "1/1 [==============================] - 0s 321ms/step\n",
      "422 [D loss: 0.002792464685626328, acc.: 100.0] [G loss: 0.22540396451950073]\n",
      "1/1 [==============================] - 0s 322ms/step\n",
      "423 [D loss: 0.00865706242620945, acc.: 100.0] [G loss: 0.243833526968956]\n",
      "1/1 [==============================] - 0s 309ms/step\n",
      "424 [D loss: 0.0035812773276120424, acc.: 100.0] [G loss: 0.13117392361164093]\n",
      "1/1 [==============================] - 0s 320ms/step\n",
      "425 [D loss: 0.002047013957053423, acc.: 100.0] [G loss: 0.1831909865140915]\n",
      "1/1 [==============================] - 0s 317ms/step\n",
      "426 [D loss: 0.004930813796818256, acc.: 100.0] [G loss: 0.1577432006597519]\n",
      "1/1 [==============================] - 0s 320ms/step\n",
      "427 [D loss: 0.0030170439276844263, acc.: 100.0] [G loss: 0.15871526300907135]\n",
      "1/1 [==============================] - 0s 306ms/step\n",
      "428 [D loss: 0.002948964713141322, acc.: 100.0] [G loss: 0.13708731532096863]\n",
      "1/1 [==============================] - 0s 315ms/step\n",
      "429 [D loss: 0.005499322549439967, acc.: 100.0] [G loss: 0.23741522431373596]\n",
      "1/1 [==============================] - 0s 324ms/step\n",
      "430 [D loss: 0.005026820581406355, acc.: 100.0] [G loss: 0.17316961288452148]\n",
      "1/1 [==============================] - 0s 314ms/step\n",
      "431 [D loss: 0.0028900281758978963, acc.: 100.0] [G loss: 0.16540037095546722]\n",
      "1/1 [==============================] - 0s 311ms/step\n",
      "432 [D loss: 0.00420644273981452, acc.: 100.0] [G loss: 0.25556302070617676]\n",
      "1/1 [==============================] - 0s 326ms/step\n",
      "433 [D loss: 0.005436049192212522, acc.: 100.0] [G loss: 0.1835310459136963]\n",
      "1/1 [==============================] - 0s 324ms/step\n",
      "434 [D loss: 0.0012602771748788655, acc.: 100.0] [G loss: 0.10674789547920227]\n",
      "1/1 [==============================] - 0s 321ms/step\n",
      "435 [D loss: 0.00843857612926513, acc.: 100.0] [G loss: 0.1273650825023651]\n",
      "1/1 [==============================] - 0s 328ms/step\n",
      "436 [D loss: 0.0014027091092430055, acc.: 100.0] [G loss: 0.16496199369430542]\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "437 [D loss: 0.0032355949515476823, acc.: 100.0] [G loss: 0.1876426637172699]\n",
      "1/1 [==============================] - 0s 322ms/step\n",
      "438 [D loss: 0.002120294579071924, acc.: 100.0] [G loss: 0.3174152970314026]\n",
      "1/1 [==============================] - 0s 310ms/step\n",
      "439 [D loss: 0.002701657242141664, acc.: 100.0] [G loss: 0.24145491421222687]\n",
      "1/1 [==============================] - 0s 316ms/step\n",
      "440 [D loss: 0.001874594483524561, acc.: 100.0] [G loss: 0.2078249156475067]\n",
      "1/1 [==============================] - 0s 321ms/step\n",
      "441 [D loss: 0.00356497336179018, acc.: 100.0] [G loss: 0.27002042531967163]\n",
      "1/1 [==============================] - 0s 326ms/step\n",
      "442 [D loss: 0.0007259112317115068, acc.: 100.0] [G loss: 0.20058463513851166]\n",
      "1/1 [==============================] - 0s 327ms/step\n",
      "443 [D loss: 0.004566767369396985, acc.: 100.0] [G loss: 0.11971963196992874]\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "444 [D loss: 0.003314308589324355, acc.: 100.0] [G loss: 0.16236723959445953]\n",
      "1/1 [==============================] - 0s 317ms/step\n",
      "445 [D loss: 0.0008165518811438233, acc.: 100.0] [G loss: 0.11718012392520905]\n",
      "1/1 [==============================] - 0s 323ms/step\n",
      "446 [D loss: 0.0019124922109767795, acc.: 100.0] [G loss: 0.1703898310661316]\n",
      "1/1 [==============================] - 0s 319ms/step\n",
      "447 [D loss: 0.0012179107288829982, acc.: 100.0] [G loss: 0.15795670449733734]\n",
      "1/1 [==============================] - 0s 306ms/step\n",
      "448 [D loss: 0.002000453299842775, acc.: 100.0] [G loss: 0.18464410305023193]\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "449 [D loss: 0.0013202503614593297, acc.: 100.0] [G loss: 0.13611634075641632]\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "450 [D loss: 0.00407122157048434, acc.: 100.0] [G loss: 0.19055745005607605]\n",
      "1/1 [==============================] - 0s 313ms/step\n",
      "451 [D loss: 0.001338407106231898, acc.: 100.0] [G loss: 0.17303580045700073]\n",
      "1/1 [==============================] - 0s 319ms/step\n",
      "452 [D loss: 0.002935688360594213, acc.: 100.0] [G loss: 0.11690482497215271]\n",
      "1/1 [==============================] - 0s 305ms/step\n",
      "453 [D loss: 0.0020301053300499916, acc.: 100.0] [G loss: 0.09019046276807785]\n",
      "1/1 [==============================] - 0s 306ms/step\n",
      "454 [D loss: 0.002793529420159757, acc.: 100.0] [G loss: 0.14892059564590454]\n",
      "1/1 [==============================] - 0s 306ms/step\n",
      "455 [D loss: 0.0017715903813950717, acc.: 100.0] [G loss: 0.14829879999160767]\n",
      "1/1 [==============================] - 0s 305ms/step\n",
      "456 [D loss: 0.002361397724598646, acc.: 100.0] [G loss: 0.1388522833585739]\n",
      "1/1 [==============================] - 0s 313ms/step\n",
      "457 [D loss: 0.0014577519614249468, acc.: 100.0] [G loss: 0.1437537968158722]\n",
      "1/1 [==============================] - 0s 319ms/step\n",
      "458 [D loss: 0.0015142021875362843, acc.: 100.0] [G loss: 0.1701742708683014]\n",
      "1/1 [==============================] - 0s 313ms/step\n",
      "459 [D loss: 0.0026653403183445334, acc.: 100.0] [G loss: 0.13437339663505554]\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "460 [D loss: 0.002164768404327333, acc.: 100.0] [G loss: 0.14373338222503662]\n",
      "1/1 [==============================] - 0s 309ms/step\n",
      "461 [D loss: 0.002683972707018256, acc.: 100.0] [G loss: 0.2351032793521881]\n",
      "1/1 [==============================] - 0s 313ms/step\n",
      "462 [D loss: 0.0010616621002554893, acc.: 100.0] [G loss: 0.14283213019371033]\n",
      "1/1 [==============================] - 0s 313ms/step\n",
      "463 [D loss: 0.004240478039719164, acc.: 100.0] [G loss: 0.12577994167804718]\n",
      "1/1 [==============================] - 0s 314ms/step\n",
      "464 [D loss: 0.0017712763510644436, acc.: 100.0] [G loss: 0.16876843571662903]\n",
      "1/1 [==============================] - 0s 317ms/step\n",
      "465 [D loss: 0.0019941835198551416, acc.: 100.0] [G loss: 0.18349717557430267]\n",
      "1/1 [==============================] - 0s 304ms/step\n",
      "466 [D loss: 0.0014141376013867557, acc.: 100.0] [G loss: 0.16221243143081665]\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "467 [D loss: 0.001505798049038276, acc.: 100.0] [G loss: 0.16859062016010284]\n",
      "1/1 [==============================] - 0s 305ms/step\n",
      "468 [D loss: 0.013006840832531452, acc.: 98.4375] [G loss: 0.19352883100509644]\n",
      "1/1 [==============================] - 0s 318ms/step\n",
      "469 [D loss: 0.0012428953777998686, acc.: 100.0] [G loss: 0.2667510509490967]\n",
      "1/1 [==============================] - 0s 302ms/step\n",
      "470 [D loss: 0.0019819310982711613, acc.: 100.0] [G loss: 0.19124072790145874]\n",
      "1/1 [==============================] - 0s 318ms/step\n",
      "471 [D loss: 0.003951281396439299, acc.: 100.0] [G loss: 0.2541014552116394]\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "472 [D loss: 0.004100035002920777, acc.: 100.0] [G loss: 0.2009732723236084]\n",
      "1/1 [==============================] - 0s 322ms/step\n",
      "473 [D loss: 0.0008108089386951178, acc.: 100.0] [G loss: 0.17781522870063782]\n",
      "1/1 [==============================] - 0s 354ms/step\n",
      "474 [D loss: 0.0010300309513695538, acc.: 100.0] [G loss: 0.1729491502046585]\n",
      "1/1 [==============================] - 0s 317ms/step\n",
      "475 [D loss: 0.0010143140680156648, acc.: 100.0] [G loss: 0.10118237137794495]\n",
      "1/1 [==============================] - 0s 315ms/step\n",
      "476 [D loss: 0.000824349801405333, acc.: 100.0] [G loss: 0.19119000434875488]\n",
      "1/1 [==============================] - 0s 316ms/step\n",
      "477 [D loss: 0.0025754832895472646, acc.: 100.0] [G loss: 0.20544736087322235]\n",
      "1/1 [==============================] - 0s 314ms/step\n",
      "478 [D loss: 0.0024194690340664238, acc.: 100.0] [G loss: 0.19388379156589508]\n",
      "1/1 [==============================] - 0s 321ms/step\n",
      "479 [D loss: 0.001495643809903413, acc.: 100.0] [G loss: 0.2032805234193802]\n",
      "1/1 [==============================] - 0s 330ms/step\n",
      "480 [D loss: 0.003726195893250406, acc.: 100.0] [G loss: 0.15921013057231903]\n",
      "1/1 [==============================] - 0s 316ms/step\n",
      "481 [D loss: 0.002793230814859271, acc.: 100.0] [G loss: 0.1691480129957199]\n",
      "1/1 [==============================] - 0s 317ms/step\n",
      "482 [D loss: 0.0015818261308595538, acc.: 100.0] [G loss: 0.17814041674137115]\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "483 [D loss: 0.0008388218120671809, acc.: 100.0] [G loss: 0.1267869472503662]\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "484 [D loss: 0.0016617455112282187, acc.: 100.0] [G loss: 0.09890371561050415]\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "485 [D loss: 0.001338238304015249, acc.: 100.0] [G loss: 0.1910146325826645]\n",
      "1/1 [==============================] - 0s 316ms/step\n",
      "486 [D loss: 0.0012324420677032322, acc.: 100.0] [G loss: 0.12396299839019775]\n",
      "1/1 [==============================] - 0s 305ms/step\n",
      "487 [D loss: 0.0006963837658986449, acc.: 100.0] [G loss: 0.2228449136018753]\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "488 [D loss: 0.0015505388146266341, acc.: 100.0] [G loss: 0.2677711844444275]\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "489 [D loss: 0.0006986784283071756, acc.: 100.0] [G loss: 0.1314963698387146]\n",
      "1/1 [==============================] - 0s 304ms/step\n",
      "490 [D loss: 0.002623366191983223, acc.: 100.0] [G loss: 0.09256623685359955]\n",
      "1/1 [==============================] - 0s 317ms/step\n",
      "491 [D loss: 0.001563011552207172, acc.: 100.0] [G loss: 0.14529988169670105]\n",
      "1/1 [==============================] - 0s 316ms/step\n",
      "492 [D loss: 0.0008301418856717646, acc.: 100.0] [G loss: 0.14019463956356049]\n",
      "1/1 [==============================] - 0s 308ms/step\n",
      "493 [D loss: 0.0015459550922969356, acc.: 100.0] [G loss: 0.11814846843481064]\n",
      "1/1 [==============================] - 0s 308ms/step\n",
      "494 [D loss: 0.0006563622591784224, acc.: 100.0] [G loss: 0.16554215550422668]\n",
      "1/1 [==============================] - 0s 329ms/step\n",
      "495 [D loss: 0.0021877615654375404, acc.: 100.0] [G loss: 0.1832471787929535]\n",
      "1/1 [==============================] - 0s 309ms/step\n",
      "496 [D loss: 0.0013099595671519637, acc.: 100.0] [G loss: 0.14124655723571777]\n",
      "1/1 [==============================] - 0s 303ms/step\n",
      "497 [D loss: 0.001887209597043693, acc.: 100.0] [G loss: 0.11729975044727325]\n",
      "1/1 [==============================] - 0s 314ms/step\n",
      "498 [D loss: 0.0014307387173175812, acc.: 100.0] [G loss: 0.11178769916296005]\n",
      "1/1 [==============================] - 0s 308ms/step\n",
      "499 [D loss: 0.0009510871896054596, acc.: 100.0] [G loss: 0.20978370308876038]\n",
      "1/1 [==============================] - 0s 306ms/step\n",
      "500 [D loss: 0.0003777800448006019, acc.: 100.0] [G loss: 0.0951455757021904]\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 364ms/step\n",
      "501 [D loss: 0.00046706513967365026, acc.: 100.0] [G loss: 0.10090270638465881]\n",
      "1/1 [==============================] - 0s 365ms/step\n",
      "502 [D loss: 0.0017055092612281442, acc.: 100.0] [G loss: 0.09087996184825897]\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "503 [D loss: 0.0010873094433918595, acc.: 100.0] [G loss: 0.09594397246837616]\n",
      "1/1 [==============================] - 0s 367ms/step\n",
      "504 [D loss: 0.001093673548894003, acc.: 100.0] [G loss: 0.09734978526830673]\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "505 [D loss: 0.002360122394748032, acc.: 100.0] [G loss: 0.11585357040166855]\n",
      "1/1 [==============================] - 0s 314ms/step\n",
      "506 [D loss: 0.0010703261650633067, acc.: 100.0] [G loss: 0.1246146559715271]\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "507 [D loss: 0.0005293619178701192, acc.: 100.0] [G loss: 0.08416232466697693]\n",
      "1/1 [==============================] - 0s 328ms/step\n",
      "508 [D loss: 0.0004078937054146081, acc.: 100.0] [G loss: 0.11669433116912842]\n",
      "1/1 [==============================] - 0s 310ms/step\n",
      "509 [D loss: 0.0009388526668772101, acc.: 100.0] [G loss: 0.0919090211391449]\n",
      "1/1 [==============================] - 0s 382ms/step\n",
      "510 [D loss: 0.0009866104810498655, acc.: 100.0] [G loss: 0.20148298144340515]\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "511 [D loss: 0.001398376189172268, acc.: 100.0] [G loss: 0.07921400666236877]\n",
      "1/1 [==============================] - 0s 323ms/step\n",
      "512 [D loss: 0.0017414209723938257, acc.: 100.0] [G loss: 0.09921761602163315]\n",
      "1/1 [==============================] - 0s 330ms/step\n",
      "513 [D loss: 0.001255720853805542, acc.: 100.0] [G loss: 0.14765781164169312]\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "514 [D loss: 0.0010903368529397994, acc.: 100.0] [G loss: 0.10044924914836884]\n",
      "1/1 [==============================] - 0s 307ms/step\n",
      "515 [D loss: 0.000960357254371047, acc.: 100.0] [G loss: 0.11805561929941177]\n",
      "1/1 [==============================] - 0s 307ms/step\n",
      "516 [D loss: 0.0037528914399445057, acc.: 100.0] [G loss: 0.1709427833557129]\n",
      "1/1 [==============================] - 0s 309ms/step\n",
      "517 [D loss: 0.0011701500334311277, acc.: 100.0] [G loss: 0.05528084933757782]\n",
      "1/1 [==============================] - 0s 325ms/step\n",
      "518 [D loss: 0.0006198581395437941, acc.: 100.0] [G loss: 0.09084649384021759]\n",
      "1/1 [==============================] - 0s 319ms/step\n",
      "519 [D loss: 0.001514058152679354, acc.: 100.0] [G loss: 0.13882562518119812]\n",
      "1/1 [==============================] - 0s 344ms/step\n",
      "520 [D loss: 0.0007472590659745038, acc.: 100.0] [G loss: 0.0742899477481842]\n",
      "1/1 [==============================] - 0s 309ms/step\n",
      "521 [D loss: 0.0018029315979219973, acc.: 100.0] [G loss: 0.09222269058227539]\n",
      "1/1 [==============================] - 0s 326ms/step\n",
      "522 [D loss: 0.0005257801531115547, acc.: 100.0] [G loss: 0.12675458192825317]\n",
      "1/1 [==============================] - 0s 319ms/step\n",
      "523 [D loss: 0.000734294910216704, acc.: 100.0] [G loss: 0.0700528621673584]\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "524 [D loss: 0.0011045433930121362, acc.: 100.0] [G loss: 0.04894525557756424]\n",
      "1/1 [==============================] - 0s 321ms/step\n",
      "525 [D loss: 0.0015188606339506805, acc.: 100.0] [G loss: 0.07332372665405273]\n",
      "1/1 [==============================] - 0s 311ms/step\n",
      "526 [D loss: 0.0010204733698628843, acc.: 100.0] [G loss: 0.1498117297887802]\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "527 [D loss: 0.0007109239522833377, acc.: 100.0] [G loss: 0.09775204956531525]\n",
      "1/1 [==============================] - 0s 328ms/step\n",
      "528 [D loss: 0.00031814866815693676, acc.: 100.0] [G loss: 0.061139918863773346]\n",
      "1/1 [==============================] - 0s 311ms/step\n",
      "529 [D loss: 0.0005886981380172074, acc.: 100.0] [G loss: 0.08884662389755249]\n",
      "1/1 [==============================] - 0s 301ms/step\n",
      "530 [D loss: 0.0014121854910627007, acc.: 100.0] [G loss: 0.10818392038345337]\n",
      "1/1 [==============================] - 0s 303ms/step\n",
      "531 [D loss: 0.0008757409814279526, acc.: 100.0] [G loss: 0.2029067873954773]\n",
      "1/1 [==============================] - 0s 306ms/step\n",
      "532 [D loss: 0.00045201220200397074, acc.: 100.0] [G loss: 0.11743181943893433]\n",
      "1/1 [==============================] - 0s 321ms/step\n",
      "533 [D loss: 0.0008276407170342281, acc.: 100.0] [G loss: 0.10324381291866302]\n",
      "1/1 [==============================] - 0s 313ms/step\n",
      "534 [D loss: 0.0008755057351663709, acc.: 100.0] [G loss: 0.07209662348031998]\n",
      "1/1 [==============================] - 0s 355ms/step\n",
      "535 [D loss: 0.0009847855544649065, acc.: 100.0] [G loss: 0.08531910181045532]\n",
      "1/1 [==============================] - 0s 298ms/step\n",
      "536 [D loss: 0.0018836737726815045, acc.: 100.0] [G loss: 0.04422622174024582]\n",
      "1/1 [==============================] - 0s 316ms/step\n",
      "537 [D loss: 0.0006458078278228641, acc.: 100.0] [G loss: 0.18617910146713257]\n",
      "1/1 [==============================] - 0s 302ms/step\n",
      "538 [D loss: 0.0008688635134603828, acc.: 100.0] [G loss: 0.12105052918195724]\n",
      "1/1 [==============================] - 0s 305ms/step\n",
      "539 [D loss: 0.0009126050281338394, acc.: 100.0] [G loss: 0.08536174148321152]\n",
      "1/1 [==============================] - 0s 318ms/step\n",
      "540 [D loss: 0.0009118511952692643, acc.: 100.0] [G loss: 0.19490377604961395]\n",
      "1/1 [==============================] - 0s 350ms/step\n",
      "541 [D loss: 0.001012143271509558, acc.: 100.0] [G loss: 0.0676121786236763]\n",
      "1/1 [==============================] - 0s 382ms/step\n",
      "542 [D loss: 0.0004511192819336429, acc.: 100.0] [G loss: 0.10498230159282684]\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "543 [D loss: 0.0009667282574810088, acc.: 100.0] [G loss: 0.12447668612003326]\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "544 [D loss: 0.0021178307360969484, acc.: 100.0] [G loss: 0.07451555132865906]\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "545 [D loss: 0.002601151616545394, acc.: 100.0] [G loss: 0.0991121307015419]\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "546 [D loss: 0.000795120547991246, acc.: 100.0] [G loss: 0.2062942236661911]\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "547 [D loss: 0.0007439595501637086, acc.: 100.0] [G loss: 0.08410877734422684]\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "548 [D loss: 0.002341643034014851, acc.: 100.0] [G loss: 0.0705791637301445]\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "549 [D loss: 0.0007124985277187079, acc.: 100.0] [G loss: 0.18748383224010468]\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "550 [D loss: 0.0008869473676895723, acc.: 100.0] [G loss: 0.08307211101055145]\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "551 [D loss: 0.00267943405197002, acc.: 100.0] [G loss: 0.06342346221208572]\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "552 [D loss: 0.0004511468578130007, acc.: 100.0] [G loss: 0.0882357507944107]\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "553 [D loss: 0.0011162390583194792, acc.: 100.0] [G loss: 0.068866066634655]\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "554 [D loss: 0.0007302833837457001, acc.: 100.0] [G loss: 0.09546414017677307]\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "555 [D loss: 0.0012288529542274773, acc.: 100.0] [G loss: 0.13811761140823364]\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "556 [D loss: 0.000845953036332503, acc.: 100.0] [G loss: 0.10194231569766998]\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "557 [D loss: 0.0010680324921850115, acc.: 100.0] [G loss: 0.08548344671726227]\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "558 [D loss: 0.00044753699330613017, acc.: 100.0] [G loss: 0.12927648425102234]\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "559 [D loss: 0.0015442187432199717, acc.: 100.0] [G loss: 0.0806131362915039]\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "560 [D loss: 0.0002805726253427565, acc.: 100.0] [G loss: 0.07359398156404495]\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "561 [D loss: 0.00030380980751942843, acc.: 100.0] [G loss: 0.09819281101226807]\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "562 [D loss: 0.0006766519945813343, acc.: 100.0] [G loss: 0.14161953330039978]\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "563 [D loss: 0.0006485524718300439, acc.: 100.0] [G loss: 0.13271686434745789]\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "564 [D loss: 0.0011514572543092072, acc.: 100.0] [G loss: 0.0693066194653511]\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "565 [D loss: 0.0007139880617614836, acc.: 100.0] [G loss: 0.05633319541811943]\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "566 [D loss: 0.0012487230123952031, acc.: 100.0] [G loss: 0.1486738622188568]\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "567 [D loss: 0.0009294230549130589, acc.: 100.0] [G loss: 0.07802356034517288]\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "568 [D loss: 0.0007140827947296202, acc.: 100.0] [G loss: 0.061781369149684906]\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "569 [D loss: 0.0006069538067094982, acc.: 100.0] [G loss: 0.0513618141412735]\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "570 [D loss: 0.0007279457058757544, acc.: 100.0] [G loss: 0.054183706641197205]\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "571 [D loss: 0.0003509154048515484, acc.: 100.0] [G loss: 0.06623297184705734]\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "572 [D loss: 0.001629512531508226, acc.: 100.0] [G loss: 0.06928304582834244]\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "573 [D loss: 0.0003418139967834577, acc.: 100.0] [G loss: 0.06651931256055832]\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "574 [D loss: 0.0005424237169791013, acc.: 100.0] [G loss: 0.1176862046122551]\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "575 [D loss: 0.0005061868869233876, acc.: 100.0] [G loss: 0.06164919584989548]\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "576 [D loss: 0.001938620931468904, acc.: 100.0] [G loss: 0.07352203130722046]\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "577 [D loss: 0.0007600475219078362, acc.: 100.0] [G loss: 0.025813501328229904]\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "578 [D loss: 0.0017746655503287911, acc.: 100.0] [G loss: 0.07595834881067276]\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "579 [D loss: 0.00042163256875937805, acc.: 100.0] [G loss: 0.08565205335617065]\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "580 [D loss: 0.0008187671191990376, acc.: 100.0] [G loss: 0.04774155467748642]\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "581 [D loss: 0.00032190681667998433, acc.: 100.0] [G loss: 0.0762798860669136]\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "582 [D loss: 0.0003407319891266525, acc.: 100.0] [G loss: 0.06484425067901611]\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "583 [D loss: 0.0009825263114180416, acc.: 100.0] [G loss: 0.04433723911643028]\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "584 [D loss: 0.0009090655948966742, acc.: 100.0] [G loss: 0.0748620331287384]\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "585 [D loss: 0.00041302839963464066, acc.: 100.0] [G loss: 0.07561555504798889]\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "586 [D loss: 0.00029366215312620625, acc.: 100.0] [G loss: 0.11751926690340042]\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "587 [D loss: 0.00044632007484324276, acc.: 100.0] [G loss: 0.0673365592956543]\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "588 [D loss: 0.0005385905387811363, acc.: 100.0] [G loss: 0.059646688401699066]\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "589 [D loss: 0.0010302476002834737, acc.: 100.0] [G loss: 0.0935739129781723]\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "590 [D loss: 0.0002701988778426312, acc.: 100.0] [G loss: 0.08553405106067657]\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "591 [D loss: 0.00018779877427732572, acc.: 100.0] [G loss: 0.11627113074064255]\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "592 [D loss: 0.005475789832416922, acc.: 100.0] [G loss: 0.08220428228378296]\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "593 [D loss: 0.00043643926619552076, acc.: 100.0] [G loss: 0.050844818353652954]\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "594 [D loss: 0.0008851070306263864, acc.: 100.0] [G loss: 0.0518469512462616]\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "595 [D loss: 0.0006786783997085877, acc.: 100.0] [G loss: 0.07590784132480621]\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "596 [D loss: 0.0005658674635924399, acc.: 100.0] [G loss: 0.055321142077445984]\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "597 [D loss: 0.0011192894307896495, acc.: 100.0] [G loss: 0.08154015243053436]\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "598 [D loss: 0.000558540690690279, acc.: 100.0] [G loss: 0.028661571443080902]\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "599 [D loss: 0.0010530430590733886, acc.: 100.0] [G loss: 0.041576094925403595]\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "600 [D loss: 0.0008057587838266045, acc.: 100.0] [G loss: 0.0790204182267189]\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "601 [D loss: 0.0005103069124743342, acc.: 100.0] [G loss: 0.1171436682343483]\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "602 [D loss: 0.0003672301536425948, acc.: 100.0] [G loss: 0.09009134769439697]\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "603 [D loss: 0.0003385129675734788, acc.: 100.0] [G loss: 0.036744266748428345]\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "604 [D loss: 0.0005207055946812034, acc.: 100.0] [G loss: 0.04509295895695686]\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "605 [D loss: 0.0005678013694705442, acc.: 100.0] [G loss: 0.03954051434993744]\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "606 [D loss: 0.0006342493652482517, acc.: 100.0] [G loss: 0.04607415944337845]\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "607 [D loss: 0.0006094206401030533, acc.: 100.0] [G loss: 0.03671407699584961]\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "608 [D loss: 0.00025343884772155434, acc.: 100.0] [G loss: 0.07926108688116074]\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "609 [D loss: 0.0008257673762273043, acc.: 100.0] [G loss: 0.0647493228316307]\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "610 [D loss: 0.0014471648319158703, acc.: 100.0] [G loss: 0.07167451828718185]\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "611 [D loss: 0.0015320352104026824, acc.: 100.0] [G loss: 0.04731770604848862]\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "612 [D loss: 0.00024111800303217024, acc.: 100.0] [G loss: 0.0453430637717247]\n",
      "1/1 [==============================] - 0s 137ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 144\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# 사용 예시\u001b[39;00m\n\u001b[0;32m    143\u001b[0m dcgan \u001b[38;5;241m=\u001b[39m DCGAN(img_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, img_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, latent_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m--> 144\u001b[0m \u001b[43mdcgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 118\u001b[0m, in \u001b[0;36mDCGAN.train\u001b[1;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[0;32m    115\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39madd(d_loss_real, d_loss_fake)\n\u001b[0;32m    117\u001b[0m noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim))\n\u001b[1;32m--> 118\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [D loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, acc.: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39md_loss[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] [G loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m save_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\kdong\\anaconda3\\envs\\working\\lib\\site-packages\\keras\\src\\engine\\training.py:2684\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2680\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2681\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2682\u001b[0m     )\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2684\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2686\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32mc:\\Users\\kdong\\anaconda3\\envs\\working\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\kdong\\anaconda3\\envs\\working\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\kdong\\anaconda3\\envs\\working\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\kdong\\anaconda3\\envs\\working\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kdong\\anaconda3\\envs\\working\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\kdong\\anaconda3\\envs\\working\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32mc:\\Users\\kdong\\anaconda3\\envs\\working\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\kdong\\anaconda3\\envs\\working\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, UpSampling2D, Conv2D, ZeroPadding2D\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "image_folder = \"C:\\\\working_space\\\\airfoil_project\\\\airfoil\"\n",
    "\n",
    "# 이미지 로드 함수\n",
    "def load_images(image_folder, image_size):\n",
    "    images = []\n",
    "    for filename in os.listdir(image_folder):\n",
    "        img = load_img(os.path.join(image_folder, filename), target_size=image_size)\n",
    "        if img is not None:\n",
    "            images.append(img_to_array(img))\n",
    "    return np.array(images)\n",
    "\n",
    "# DCGAN 클래스 정의\n",
    "class DCGAN:\n",
    "    def __init__(self, img_rows, img_cols, channels, latent_dim):\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        z = tf.keras.layers.Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 16 * 16, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((16, 16, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        noise = tf.keras.layers.Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        img = tf.keras.layers.Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        X_train = load_images(image_folder, (self.img_rows, self.img_cols))\n",
    "        X_train = X_train / 127.5 - 1.0\n",
    "\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}] [G loss: {g_loss}]\")\n",
    "\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(f\"images/dcgan_{epoch}.png\")\n",
    "        plt.close()\n",
    "\n",
    "# 사용 예시\n",
    "dcgan = DCGAN(img_rows=64, img_cols=64, channels=3, latent_dim=100)\n",
    "dcgan.train(epochs=10000, batch_size=32, save_interval=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
